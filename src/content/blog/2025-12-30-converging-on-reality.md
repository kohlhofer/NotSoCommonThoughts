---
title: "Converging on Reality"
description: "Machine learning models across languages, molecules, and proteins are converging toward similar representations—a hint at underlying structure."
date: 2025-12-30
tags: ["Artificial Intelligence", "Reflections"]
featured: false
---

From [Ethan Mollick on LinkedIn](https://www.linkedin.com/posts/emollick_recently-llms-were-found-to-encode-different-activity-7411757757713907713-QjVu):

> Recently, LLMs were found to encode different languages in similar ways, suggesting a sort of Platonic representation of words.
>
> It now extends to science: 60 ML models for molecules, materials & proteins (all with different architectures & training data) converge toward similar encodings of molecular structure.
>
> As models improve, they agree more on what makes two molecules "similar." They may be converging on a shared representation of physical reality (but models are still constrained by their data).

I find this a fascinating hint at an underlying structure of information and reality.

Give [Ethan](https://www.linkedin.com/in/emollick/) a follow—he's a consistently considerate and often humorous source of insights on all things AI.
